{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"APEx Dispatch API","text":"<p>Welcome to the APEx Dispatch API documentation!</p> <p>The APEx Dispatch API is a powerful service designed to execute and upscale Earth Observation (EO) services using APEx-compliant technologies. It provides a unified interface for running EO-based services across various cloud platforms, regardless of the underlying infrastructure.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"<p>The API currently supports the execution of the following types of EO services:</p> <ul> <li>openEO-based services: These are workflows built using the openEO standard.</li> <li>OGC API Processes: Services that conform to the OGC API Processes standard.</li> </ul> <p>In addition to executing individual services, the APEx Dispatch API also supports upscaling operations. This means that large areas of interest can be automatically divided into smaller, manageable chunks, each processed as a separate job. The API handles the orchestration and coordination of these jobs, simplifying the management of complex and large-scale processing tasks.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>The APEx Dispatch API acts as a broker service that allows clients to trigger job executions on external Earth Observation platforms. Instead of interacting directly with platform-specific APIs, clients can use the uniform Dispatch API interface, while the dispatcher handles the translation and job management.</p>"},{"location":"architecture/#key-concepts","title":"Key Concepts","text":""},{"location":"architecture/#dispatch-api","title":"Dispatch API","text":"<p>The Dispatch API is the core component of the system. It acts as the entry point for clients who want to execute jobs or perform upscaling tasks. When a job request is submitted, the dispatcher takes care of translating it into a platform-specific request using standards such as openEO or OGC API \u2013 Processes that is sent to an existing EO platform, such as CDSE or the Geohazard Exploitation Platform. Beyond handling the translation, the dispatcher is also responsible for storing all relevant job information, including metadata and references to the external platform where the job is ultimately executed.</p> <pre><code>flowchart LR\n    C[\"Client\"] --&gt; D[\"APEx Dispatch API\"]\n    D-.-&gt;C\n    D--&gt;P1[Platform 1]\n    P1-.-&gt;D\n    D--&gt;P2[Platform 2]\n    P2-.-&gt;D\n    D--&gt;P3[Platform 3]\n    P3-.-&gt;D</code></pre>"},{"location":"architecture/#processing-job-execution","title":"Processing Job Execution","text":"<p>When a client wants to perform a task, it submits a job to the Dispatch API. A job request typically contains two main pieces of information: the service that needs to be executed and the parameters required for that service. Once received, the dispatcher forwards this request to the chosen external platform, which carries out the execution. In response, the platform provides a job identifier, which the dispatcher records internally to keep track of the execution.</p> <p>After a job has been submitted and forwarded to an external platform, the dispatcher maintains an internal record of it. This record includes a unique internal job identifier, which the client can use for reference, as well as the mapping to the external platform\u2019s job ID. Additional metadata, such as the job status, the creation timestamp, and the parameters used during submission, are also stored. This internal tracking mechanism ensures that the client has a single point of reference for all jobs, regardless of where they are executed.</p> <pre><code>sequenceDiagram\n    participant UI as Client\n    box APEx\n    participant API as APEx Dispatch API\n    end\n    box Platform\n    participant Platform as API (openEO / OGC API Process)\n    end\n\n    UI-&gt;&gt;API: POST /unit_jobs\n\n    API-&gt;&gt;API: Create processing job\n    API-&gt;&gt;Platform: Submit processing job\n    Platform--&gt;&gt;API: Return platform job ID\n    API-&gt;&gt;API:Store platform job ID \n    API-&gt;&gt;API:Set job status as \"submitted\"\n\n    API--&gt;&gt;UI: Return processing job summary</code></pre>"},{"location":"architecture/#upscaling-task-execution","title":"Upscaling Task Execution","text":"<p>In addition to individual job submissions, the dispatcher also supports upscaling activities. In this case, a client submits a request that includes not just the target service and execution parameters, but also a parameter dimension with multiple values. The dispatcher uses this information to generate multiple job requests, each corresponding to one value in the parameter dimension, and forwards them to the external platform. From the client\u2019s perspective, however, this entire batch of jobs is managed as a single upscaling task. The dispatcher keeps track of the execution of all related jobs and exposes them as part of one unified task, simplifying monitoring and retrieval for the user.</p> <pre><code>sequenceDiagram\n    participant UI as Client\n    box APEx\n    participant API as APEx Dispatch API\n    end\n    box Platform\n    participant Platform as API (openEO / OGC API Process)\n    end\n\n    UI-&gt;&gt;API: POST /upscale_tasks\n\n    API-&gt;&gt;API: Create upscaling task\n\n\n    loop For each job of upscaling task\n    API-&gt;&gt;API: Create processing job\n    API-&gt;&gt;Platform: Submit processing job\n    Platform--&gt;&gt;API: Return platform job ID\n    API-&gt;&gt;API:Store platform job ID \n    API-&gt;&gt;API:Set job status as \"submitted\"\n    end\n\n    API--&gt;&gt;UI: Return upscaling task summary</code></pre>"},{"location":"architecture/#status-retrieval","title":"Status Retrieval","text":"<p>To check the progress of their jobs and upscale tasks, clients use a single status endpoint exposed by the Dispatch API. When such a request arrives, the dispatcher looks up the corresponding external job reference stored in its internal records. It then queries the external platform to obtain the most up-to-date status. This status information is returned to the client, allowing them to monitor their job execution transparently through the dispatcher without needing to interact with the external platform directly.</p> <pre><code>sequenceDiagram\n    participant UI as Client\n    box APEx\n    participant API as APEx Dispatch API\n    end\n    box Platform\n    participant Platform as API (openEO / OGC API Process)\n    end\n\n    UI-&gt;&gt;+API: Set up websocket to /job_status\n\n    loop Every X minutes\n        loop For each running processing job\n            API-&gt;&gt;Platform: Request job status\n            Platform--&gt;&gt;API: Send job status\n            API-&gt;&gt;API: Update job status\n        end\n\n        loop For each running upscaling task\n            loop For each running processing job in upscaling task\n                API-&gt;&gt;Platform: Request job status\n                Platform--&gt;&gt;API: Send job status\n                API-&gt;&gt;API: Update job status\n            end\n            API-&gt;&gt;API: Compute upscaling task status\n        end\n    end\n    API--&gt;&gt;-UI: Return summary list of processing jobs and upscaling tasks</code></pre>"},{"location":"architecture/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>Authentication and authorization are critical components of the APEx Dispatch API, as jobs launched through the API result in resource consumption on external platforms. To support remote job execution and manage this resource usage effectively, the project has identified two distinct scenarios:</p>"},{"location":"architecture/#apex-service-account-current-implementation","title":"APEx Service Account (Current Implementation)","text":"<p>In this scenario, all jobs are executed on the external platforms using a generic APEx service account that has access to them. This means that each job or upscaling task triggered through the API is executed on the platform under the APEx account, rather than the actual user\u2019s identity. However, the Dispatch API maintains the link between the platform job ID and the user who initiated the request in its database.</p> <pre><code>flowchart LR\n    C[\"Alice\"] -- Request job---&gt; D[\"APEx Dispatch API\"]\n    D--Launch job as user APEx --&gt;P1[Platform ]</code></pre> <p>Pros:</p> <ul> <li>Provides a seamless user experience: users do not need to create or manage platform-specific accounts.</li> <li>Simplifies integration for clients using the Dispatch API.</li> </ul> <p>Cons:</p> <ul> <li>For each new platform, a dedicated APEx account must be created and funded appropriately. Estimating the required funding in advance is challenging, especially since the account is shared across all users.</li> <li>No user-level auditing or accounting is available. Users can continue triggering jobs as long as the APEx account has sufficient funds. This poses a risk of misuse, potentially leading to service disruption for all users if the APEx account is depleted.</li> <li>Implementing safeguards would require advanced accounting features within the Dispatch API, requiring the translation of existing business models into a uniform business logic. This adds complexity and may introduce additional costs by layering over existing platform models.</li> </ul>"},{"location":"architecture/#user-impersonation-preferred-approach","title":"User Impersonation (Preferred Approach)","text":"<p>The preferred solution is to execute jobs on behalf of the user who initiates the request via the APEx Dispatch API. In this model, all accounting and access control are handled directly by the platform, and users are responsible for maintaining sufficient access and funding\u2014potentially supported through the ESA Network of Resources (NoR).</p> <pre><code>flowchart LR\n    C[\"Alice\"] -- Request job---&gt; D[\"APEx Dispatch API\"]\n    D--Launch job as user Alice --&gt;P1[Platform ]</code></pre> <p>Pros:</p> <ul> <li>No need for custom accounting logic in the APEx Dispatch API, as platforms handle this natively.</li> <li>APEx avoids introducing a layer over the platform\u2019s existing business model, preserving operational simplicity.</li> </ul> <p>Cons:</p> <ul> <li>Propagating user identity across platforms is a technical challenge and currently lacks a proven, ready-to-use solution.</li> <li>May require modifications on the target platform to support user impersonation, depending on the chosen implementation strategy.</li> </ul>"},{"location":"configuration/","title":"Configuring the Dispatcher","text":"<p>The Dispatcher can be configured using environment variables. These variables can be set directly in your shell or defined in a <code>.env</code> file for convenience. Below are the key settings that can be adjusted to tailor the Dispatcher's behavior to your needs. </p> Environment Variable Description Values Default Value General Settings <code>APP_NAME</code> The name of the application. Text APEx Dispatch API <code>APP_DESCRIPTION</code> A brief description of the application. Text \"\" <code>APP_ENV</code> The environment in which the application is running <code>development</code> /  <code>production</code> development <code>CORS_ALLOWED_ORIGINS</code> Comma-separated list of allowed origins for CORS. Text \"\" Database Settings <code>DATABASE_URL</code> The database connection URL. Text \"\" Keycloak Settings <code>KEYCLOAK_HOST</code> The hostname of the Keycloak server. Text localhost <code>KEYCLOAK_REALM</code> The Keycloak realm to use for authentication. Text \"\" <code>KEYCLOAK_CLIENT_ID</code> The client ID registered in Keycloak. Text \"\" <code>KEYCLOAK_CLIENT_SECRET</code> The client secret for the Keycloak client. Text \"\" openEO Settings <code>OPENEO_AUTH_METHOD</code> The authentication method to use for openEO backends. <code>USER_CREDENTIALS</code> / <code>CLIENT_CREDENTIALS</code> <code>USER_CREDENTIALS</code> <code>OPENEO_BACKEND_CONFIG</code> JSON string defining the configuration for openEO backends. JSON <code>{}</code>"},{"location":"configuration/#openeo-backend-configuration","title":"openEO Backend Configuration","text":"<p>The <code>OPENEO_BACKEND_CONFIG</code> environment variable allows you to specify the configuration for multiple openEO backends in JSON format. Here is an example of how to structure this configuration:  </p> <p><pre><code>{\n  \"https://openeo.backend1.com\": {\n    \"auth_method\": \"CLIENT_CREDENTIALS\",\n    \"client_credentials\": \"oidc_provider/client_id/secret_secret\",\n  },\n \"https://openeo.backend2.com\": {\n    \"auth_method\": \"USER_CREDENTIALS\",\n    \"token_provider\": \"backend\",\n    \"token_prefix\": \"oidc/backend\"\n  }, \n  ...\n}\n</code></pre> Each backend configuration can include the following fields:</p> <ul> <li><code>auth_method</code>: The authentication method to use for the openEO backend. This value can either be <code>USER_CREDENTIALS</code> or <code>CLIENT_CREDENTIALS</code>. The default value is set to <code>USER_CREDENTIALS</code>.</li> <li><code>client_credentials</code>: The client credentials for authenticating with the openEO backend. This is required if the <code>OPENEO_AUTH_METHOD</code> is set to <code>CLIENT_CREDENTIALS</code>. It is a single string in the format <code>oidc_provider/client_id/client_secret</code> that should be split into its components when used.</li> <li><code>token_provider</code>: The provider refers to the OIDC IDP alias that needs to be used to exchange the incoming token to an external token. This is required if the <code>OPENEO_AUTH_METHOD</code> is set to <code>USER_CREDENTIALS</code>. For example, if you have a Keycloak setup with an IDP alias <code>openeo-idp</code>, you would set this field to <code>openeo-idp</code>. This means that when a user authenticates with their token, the Dispatcher will use the <code>openeo-idp</code> to exchange the user's token for a token that is valid for the openEO backend.</li> <li><code>token_prefix</code>: An optional prefix to be added to the token when authenticating (e.g., \"CDSE\"). The prefix is required by some backends to identify the token type. This will be prepended to the exchanged token when authenticating with the openEO backend.</li> </ul>"},{"location":"configuration/#example-configuration","title":"Example Configuration","text":"<p>Here is an example of setting the environment variables in a <code>.env</code> file:</p> <pre><code># General Settings\nAPP_NAME=\"APEx Dispatch API\"\nAPP_DESCRIPTION=\"APEx Dispatch Service API to run jobs and upscaling tasks\"\nAPP_ENV=development\n\nCORS_ALLOWED_ORIGINS=http://localhost:5173\n\n# Database Settings\nDATABASE_URL=sqlite:///:memory:\n\n# Keycloak Settings\nKEYCLOAK_HOST=localhost\nKEYCLOAK_REALM=apex\nKEYCLOAK_CLIENT_ID=apex-client-id\nKEYCLOAK_CLIENT_SECRET=apex-client-secret\n\n\n# openEO Settings\nOPENEO_AUTH_METHOD=USER_CREDENTIALS\nOPENEO_BACKENDS='{\"https://openeo.backend1.com\" {\"auth_method\": \"CLIENT_CREDENTIALS\", \"client_credentials\": \"oidc_provider/client_id/secret_secret\"}, \"https://openeo.backend2.com\" {\"auth_method\": \"USER_CREDENTIALS\",  \"token_provider\": \"backend\", \"token_prefix\": \"oidc/backend\"}}'\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#making-contributions","title":"Making Contributions","text":"<p>Contributions to the APEx Dispatch API are welcome! If you have suggestions for improvements, bug fixes, or new features, please follow these steps:</p> <ol> <li>Fork the repository: Create a personal copy of the repository on GitHub.</li> <li>Create a new branch: Use a descriptive name for your branch that reflects the changes you plan to make.    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes: Implement the changes you want to contribute.</li> <li>Write tests: Ensure that your changes are covered by tests. Add new tests if necessary.</li> <li>Run tests: Verify that all tests pass before submitting your changes.    <pre><code>pytest\n</code></pre></li> <li>Commit your changes: Write a clear commit message that describes your changes.    <pre><code>git commit -m \"Add feature: your feature description\"\n</code></pre></li> <li>Push your changes: Push your branch to your forked repository.    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Create a pull request: Go to the original repository and create a pull request (PR) from your branch. Provide a clear description of the changes and why they are needed.</li> </ol>"},{"location":"contributing/#registration-of-a-new-platform-implementation","title":"Registration of a new Platform Implementation","text":"<p>To add a new platform implementation, you will need to create a new class that inherits from the <code>BaseProcessingPlatform</code> class located at <code>app/platforms/base.py</code>. In this new class, you will need to implement all the abstract methods defined in the <code>BaseProcessingPlatform</code> class. This will ensure that your new platform implementation adheres to the expected interface and functionality.</p> <p>To register the new implementation, it is important to add the following directive right above the class definition:</p> <pre><code>from app.platforms.dispatcher import register_platform\nfrom app.schemas.enum import ProcessTypeEnum\n\n@register_platform(ProcessTypeEnum.OGC_API_PROCESS)\nclass OGCAPIProcessPlatform(BaseProcessingPlatform):\n    ...\n</code></pre> <p>The processing type, defined by <code>ProcessTypeEnum</code>, is the unique identifier for the platform implementation. It is used to distinguish between different platform implementations in the system. This value is used by the different request endpoints to determine which platform implementation to use for processing the request. To add a new platform implementation, you will need to define a new <code>ProcessTypeEnum</code> value in the <code>app/schemas/enum.py</code> file. This value should be unique and descriptive of the platform you are implementing.</p> <p>Once you have completed the above steps, the new platform implementation will be registered automatically and made available for use in the APEx Dispatch API. You can then proceed to implement the specific functionality required for your platform.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before running the APEx Dispatch API locally, ensure the following prerequisites are met:</p> <ul> <li>A working Python environment to install and run the API dependencies.</li> <li>A PostgreSQL database to store job-related information.</li> </ul>"},{"location":"getting_started/#running-the-api-on-your-local-environment","title":"Running the API on your local environment","text":"<p>Follow these steps to set up and run the API in your local development environment:</p>"},{"location":"getting_started/#install-dependencies","title":"Install dependencies","text":"<p>Use <code>pip</code> to install the required Python packages:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting_started/#set-up-the-database","title":"Set up the database","text":"<p>To set up a PostgreSQL database locally, you can use Docker for convenience and persistence.</p>"},{"location":"getting_started/#optional-create-a-docker-volume","title":"(Optional) Create a Docker Volume","text":"<p>This step ensures your PostgreSQL data is stored persistently:</p> <pre><code>docker volume create local-postgres-data\n</code></pre> <p>To view the physical location of the volume on your host machine:</p> <pre><code>docker volume inspect local-postgres-data\n</code></pre>"},{"location":"getting_started/#start-a-postgresql-container","title":"Start a PostgreSQL Container","text":"<p>Run the following command to start a PostgreSQL instance linked to your volume:</p> <pre><code>docker run -d --name postgres -p 5432:5432 \\\n    -e POSTGRES_USER=testuser \\\n    -e POSTGRES_PASSWORD=secret \\\n    -e POSTGRES_DB=testdb \\\n    -v local-postgres-data:/var/lib/docker/volumes/local-postgres-data \\\n    postgres:latest\n</code></pre>"},{"location":"getting_started/#configure-the-environment","title":"Configure the environment","text":"<p>Create a <code>.env</code> file in the root directory of the project and set the necessary environment variables as described in the Environment Configuration documentation.</p>"},{"location":"getting_started/#apply-database-migrations","title":"Apply Database Migrations","text":"<p>Ensure your database schema is up-to-date by running:</p> <pre><code>alembic upgrade head\n</code></pre>"},{"location":"getting_started/#running-the-api","title":"Running the API","text":"<p>To start the API, open a new terminal and execute the following:</p> <pre><code>uvicorn app.main:app --reload\n</code></pre> <p>The API will be available at:</p> <ul> <li>Swagger UI: http://127.0.0.1:8000/docs</li> </ul>"},{"location":"getting_started/#running-tests","title":"Running Tests","text":"<p>Testing is essential to ensure stability and prevent regression issues from affecting the functionality of the API. This project includes a comprehensive suite of tests to validate its core features and maintain code quality.</p>"},{"location":"getting_started/#unit-testing","title":"Unit Testing","text":"<p>The repository contains an extensive collection of unit tests that cover the main components and functionalities of the codebase. To execute the test suite, run:</p> <pre><code>pytest\n</code></pre> <p>This will automatically discover and run all tests located in the designated test directories.</p>"},{"location":"getting_started/#linting","title":"Linting","text":"<p>To maintain consistent code quality and enforce best practices, the project uses <code>flake8</code> for linting and <code>mypy</code> for static type checking. You can run these tools manually with the following commands:</p> <pre><code>flake8 app tests\nmypy app\n</code></pre> <p>These checks help identify potential issues early, such as syntax errors, unused imports, and type mismatches, contributing to a more robust and maintainable codebase.</p>"},{"location":"performance/","title":"Executing Performance Tests","text":"<p>This repository includes tools to execute performance tests for the APEx Dispatch API. Performance testing is useful for analyzing the impact of code changes, database updates, and platform modifications on the system's behavior and responsiveness.</p>"},{"location":"performance/#prerequisites","title":"Prerequisites","text":"<p>Before running the performance tests, ensure the following prerequisites are met:</p> <ul> <li>Python environment (Python 3.10+ recommended)</li> <li>Docker and Docker Compose installed on your system</li> </ul>"},{"location":"performance/#setting-up-the-environment","title":"Setting Up the Environment","text":"<p>Performance tests require both the API and a database to be running locally. Follow these steps to set up your environment:</p> <ol> <li> <p>Create a <code>.env</code> file in the root of the repository with the following variables:</p> </li> <li> <p><code>OPENEO_BACKENDS_PERFORMANCE</code> \u2192 Configuration for the openEO backend authentication. See configuration guide.</p> </li> <li><code>KEYCLOAK_CLIENT_PERFORMANCE_ID</code> \u2192 Client ID used for executing performance tests.</li> <li> <p><code>KEYCLOAK_CLIENT_PERFORMANCE_SECRET</code> \u2192 Client secret used for executing performance tests.</p> </li> <li> <p>Start the services using Docker Compose:</p> </li> </ol> <pre><code>docker compose -f docker-compose.perf.yml up -d db\n</code></pre> <p>Starts a local database instance.</p> <pre><code>docker compose -f docker-compose.perf.yml up -d migrate\n</code></pre> <p>Executes database migrations to ensure all required tables are created.</p> <pre><code>docker compose -f docker-compose.perf.yml up -d app\n</code></pre> <p>Starts the API locally.</p> <p>Tip: You can check the logs of each service with <code>docker compose -f docker-compose.perf.yml logs -f &lt;service_name&gt;</code>.</p>"},{"location":"performance/#executing-performance-tests_1","title":"Executing Performance Tests","text":"<p>The performance tests are implemented using Locust. Test scenarios are located in <code>tests/performance/locustfile.py</code>.</p>"},{"location":"performance/#running-tests-with-a-web-dashboard","title":"Running Tests with a Web Dashboard","text":"<p>To execute the performance tests and monitor them in a browser dashboard:</p> <pre><code>locust -f tests/performance/locustfile.py -u 10 --host http://localhost:8000 --run-time 1m\n</code></pre> <ul> <li><code>-u 10</code> \u2192 Number of simulated concurrent users</li> <li><code>--host http://localhost:8000</code> \u2192 URL of the API to test</li> <li><code>--run-time 1m</code> \u2192 Duration of the test</li> </ul> <p>After starting, open your browser at http://localhost:8089 to monitor real-time performance metrics, including response times, failure rates, and throughput.</p>"},{"location":"performance/#running-tests-in-headless-mode","title":"Running Tests in Headless Mode","text":"<p>To execute tests without a web interface (useful for CI/CD pipelines):</p> <pre><code>locust -f tests/performance/locustfile.py -u 10 --host http://localhost:8000 --run-time 1m --headless\n</code></pre> <p>You can also export the results to a CSV file for further analysis:</p> <pre><code>locust -f tests/performance/locustfile.py -u 10 --host http://localhost:8000 --run-time 1m --headless --csv=perf_test_results\n</code></pre>"},{"location":"performance/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Start with a small number of users to validate test scripts before scaling up.</li> <li>Combine performance testing with monitoring tools to detect resource bottlenecks.</li> </ul>"}]}